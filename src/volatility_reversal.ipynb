{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Volatility Reversals Based on Daily News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: \n",
    "\n",
    "To fine-tune an LLM that can accurately classify volatility reversals by analysing daily news headlines, helping traders and investors make more informed decisions about managing risk and timing trades.\n",
    "\n",
    "Volatility reversals are critical events in financial markets, often signaling significant changes in market sentiment and potential shifts in asset prices. Accurately identifying these reversals enables traders and analysts to anticipate sudden shifts in market direction, improving risk management and trading strategies. By identifying potential reversals, investors can optimize entry and exit points, reduce losses, and enhance returns. Additionally, accurate classification helps in developing algorithmic trading models that adapt to changing market conditions, ultimately leading to more resilient and efficient trading systems.\n",
    "\n",
    "Rationale:\n",
    "\n",
    "Traders and investors are constantly being bombarded with an endless stream of news and information, some of which hold contrarian or opposing views from the current market conditions. However, traders and investors do not usually act upon 1 piece of news alone, but rather the overall market sentiment. This is where the skill of the individual comes into play; knowing exactly when the current market sentiment has shifted, and what actions to take next. I want to identify moments when negative or contrarian views, which may have been wrong for a while, start to get serious market attention and actually cause a volatility reverse. I later define what constitutes volatility reversal below. Being a data scientist, instead of relying on years of trading experience, I shall create and fine-tune a model to emulate the knowledge of experts in the finance industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datetime import timedelta, datetime\n",
    "# from IPython.display import HTML\n",
    "# import refinitiv.data as rd\n",
    "import yfinance as yf\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from google.colab import drive\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection - Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to Refinitiv Data Platform obtained by requesting for an account via NUS Business Financial Database.\n",
    "https://www.lseg.com/en/data-analytics/products/workspace\n",
    "\n",
    "Data is obtained between the dates 2023-11-15 to 2025-02-13. 2023-11-15 is the earliest date that headlines could be retrieved, any earlier returns NaN.\n",
    "\n",
    "I queried for headlines relevant to S&P as the volatility index I will be using, VIX, tracks the volatility of the S&P. I also set a filter for English headlines. It is possible to translate non-English headlines using Google Translator API, however due to time constraints, this step has been forgone.\n",
    "\n",
    "All news headlines for a given day are then aggregating into a single string. The rationale for this is explained below in the feature engineering section. However, the code is included here as the aggregation was done of the Refinitiv Data Platform itself before extracting the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Start session\n",
    "rd.open_session()\n",
    "\n",
    "headlines = pd.DataFrame()\n",
    "file_num = 1\n",
    "# Get headlines for past 460 days (2023-11-15 to 2025-02-13). Checked that 2023-11-15 is earliest date available.\n",
    "for i in range(460):\n",
    "    date = datetime.strptime('2025-02-13', '%Y-%m-%d') + timedelta(days=-i)\n",
    "    print(date)\n",
    "    # Actually query for headlines here\n",
    "    daily_headlines = rd.news.get_headlines(\"S&P AND Language:LEN\", start=date, end=date + timedelta(days=1), count=700)\n",
    "    # Combine all headlines for the day into one string using [SEP] as separator\n",
    "    daily_combined_headlines = daily_headlines.iloc[:, 0].astype(str).str.cat(sep=\"[SEP]\")\n",
    "    # Store the data in a DataFrame\n",
    "    df_daily = pd.DataFrame([{\"date\": date, \"combined_headlines\": daily_combined_headlines}])\n",
    "    headlines = pd.concat([headlines, df_daily], axis=0)\n",
    "    # Save the data every 50 days since it takes a long time to get all the data, occasionally timing out\n",
    "    if i % 50==0 and i!=0:\n",
    "        headlines.to_csv(f\"headlines_{file_num}.csv\", encoding='utf-8', index=False)\n",
    "        headlines = pd.DataFrame()\n",
    "        print(f\"headlines_{file_num}.csv ABOVE\")\n",
    "        file_num += 1\n",
    "# Save the remaining data\n",
    "headlines.to_csv(f\"headlines_{file_num}.csv\", encoding='utf-8', index=False)\n",
    "print(f\"headlines_{file_num}.csv ABOVE\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection - Volatility Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily VIX data from yfinance. Dates are matched to days with headlines from Refinitiv Data Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-15</th>\n",
       "      <td>14.180000</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>13.97</td>\n",
       "      <td>14.210000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-16</th>\n",
       "      <td>14.320000</td>\n",
       "      <td>14.420000</td>\n",
       "      <td>13.68</td>\n",
       "      <td>14.120000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-17</th>\n",
       "      <td>13.800000</td>\n",
       "      <td>14.190000</td>\n",
       "      <td>13.67</td>\n",
       "      <td>14.180000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-20</th>\n",
       "      <td>13.410000</td>\n",
       "      <td>14.310000</td>\n",
       "      <td>13.39</td>\n",
       "      <td>14.260000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-21</th>\n",
       "      <td>13.350000</td>\n",
       "      <td>14.310000</td>\n",
       "      <td>13.13</td>\n",
       "      <td>13.450000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-07</th>\n",
       "      <td>16.540001</td>\n",
       "      <td>16.660000</td>\n",
       "      <td>14.79</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-10</th>\n",
       "      <td>15.810000</td>\n",
       "      <td>16.610001</td>\n",
       "      <td>15.70</td>\n",
       "      <td>16.580000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-11</th>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.420000</td>\n",
       "      <td>15.75</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-12</th>\n",
       "      <td>15.890000</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>15.64</td>\n",
       "      <td>15.910000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-13</th>\n",
       "      <td>15.100000</td>\n",
       "      <td>16.330000</td>\n",
       "      <td>14.98</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Close       High    Low       Open  Volume\n",
       "Date                                                      \n",
       "2023-11-15  14.180000  14.350000  13.97  14.210000       0\n",
       "2023-11-16  14.320000  14.420000  13.68  14.120000       0\n",
       "2023-11-17  13.800000  14.190000  13.67  14.180000       0\n",
       "2023-11-20  13.410000  14.310000  13.39  14.260000       0\n",
       "2023-11-21  13.350000  14.310000  13.13  13.450000       0\n",
       "2025-02-07  16.540001  16.660000  14.79  15.380000       0\n",
       "2025-02-10  15.810000  16.610001  15.70  16.580000       0\n",
       "2025-02-11  16.020000  16.420000  15.75  16.129999       0\n",
       "2025-02-12  15.890000  17.180000  15.64  15.910000       0\n",
       "2025-02-13  15.100000  16.330000  14.98  15.970000       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get VIX data from yfinance\n",
    "vix_data = yf.download(\"^VIX\", start='2023-11-15', end='2025-02-14')\n",
    "\n",
    "# Save VIX data to CSV\n",
    "vix_data.columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"] # Rename columns\n",
    "vix_data.to_csv(\"../data/VIX_data.csv\") # Save to CSV\n",
    "\n",
    "# Display VIX data\n",
    "pd.concat([vix_data.head(), vix_data.tail()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All news headlines for a given day are then aggregating into a single string. Doing this has a number of benefits over classifying each individual headline.\n",
    "1. Market Sentiment Context: Financial markets react to the overall sentiment of the day's news rather than individual headlines. A single news article might not capture the full picture, but aggregating all headlines provides a more comprehensive view of market sentiment.\n",
    "2. Noise Reduction: Individual headlines can be misleading or lack sufficient context for accurate classification, especially contrarian views that turn out to be wrong. By aggregating headlines, outliers are smoothed out, reducing the impact of sensational or less relevant news. This provides a more robust and stable representation of the day's sentiment compared to classifying and averaging individual headlines, which may introduce unnecessary variance.\n",
    "\n",
    "When aggregating the headlines, I used the separator [SEP] to preserve each headlines' boundaries, preventing them from merging into an indistinguishable block of text. This helps models recognize distinct pieces of information.\n",
    "\n",
    "It takes a long time to query all the data and the connection might time out, causing me to lose data. I split up the output file by saving every 50 queries allowing me to continue from when the connection was lost. Restarting the full query also faced the problem of hitting the API request limit. \n",
    "\n",
    "However, I realised that the length of the combined daily headlines likely exceeds the token limit for FinBERT. In order to circumvent this, I used a chunking approach, splitting the daily combined headlines into 100 concatenated headlines at once. I chose chunking over classifying each individual headline in order to preserves context across multiple headlines. Additionally, each chunk within a day likely follow similar distributions. This chunking is done under the Feature Engineering - Merging section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Volatility Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heuristic classifies daily movements in the VIX (CBOE Volatility Index) into `Positive` Reversal, `Negative` Reversal, or `No Reversal` based on specific conditions.\n",
    "\n",
    "This classification will later be used as the labels for the the news headlines.\n",
    "\n",
    "1. Compute Daily Changes\n",
    "\n",
    "Let `Close` be the closing price of VIX on a given day.\n",
    "\n",
    "Define `PrevClose` as the previous day's closing price.\n",
    "\n",
    "Define `PrevClose_2` as the closing price two days prior.\n",
    "\n",
    "Compute the daily percentage change in VIX:\n",
    "\n",
    "`Change = (Close − PrevClose) / PrevClose`\n",
    " \n",
    "Compute the previous day's percentage change (`PrevChange`).\n",
    "\n",
    "2. Define Reversal Conditions\n",
    "\n",
    "Positive Reversal:\n",
    "\n",
    "The previous day’s closing price was higher than two days ago (`PrevClose > PrevClose_2`).\n",
    "\n",
    "The current closing price is lower than the previous day (`Close < PrevClose`).\n",
    "\n",
    "The previous day's change (`PrevChange`) was greater than +5% (`PrevChange > 0.05`).\n",
    "\n",
    "Negative Reversal:\n",
    "\n",
    "The previous day’s closing price was lower than two days ago (`PrevClose < PrevClose_2`).\n",
    "\n",
    "The current closing price is higher than the previous day (`Close > PrevClose`).\n",
    "\n",
    "The previous day's change (`PrevChange`) was less than -5% (`PrevChange < -0.05`).\n",
    "\n",
    "3. Final Classification\n",
    "\n",
    "If Positive Reversal conditions are met → Label as \"`Positive`\".\n",
    "\n",
    "If Negative Reversal conditions are met → Label as \"`Negative`\".\n",
    "\n",
    "Otherwise → Label as \"`No Reversal`\".\n",
    "\n",
    "This approach ensures that only significant trend reversals (where the previous day's movement exceeded ±5%) are captured, filtering out minor fluctuations. The results are saved as a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Reversal_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>14.180000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>14.320000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-20</td>\n",
       "      <td>13.410000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>16.540001</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2025-02-10</td>\n",
       "      <td>15.810000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>15.890000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>No Reversal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Close Reversal_Label\n",
       "0    2023-11-15  14.180000    No Reversal\n",
       "1    2023-11-16  14.320000    No Reversal\n",
       "2    2023-11-17  13.800000    No Reversal\n",
       "3    2023-11-20  13.410000    No Reversal\n",
       "4    2023-11-21  13.350000    No Reversal\n",
       "307  2025-02-07  16.540001    No Reversal\n",
       "308  2025-02-10  15.810000       Positive\n",
       "309  2025-02-11  16.020000    No Reversal\n",
       "310  2025-02-12  15.890000    No Reversal\n",
       "311  2025-02-13  15.100000    No Reversal"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "vix_data = pd.read_csv(\"../data/VIX_data.csv\")  # Ensure dataset has 'Date' and 'Close'\n",
    "\n",
    "# Compute daily metrics\n",
    "vix_data['PrevClose'] = vix_data['Close'].shift(1)\n",
    "vix_data['PrevClose_2'] = vix_data['Close'].shift(2)\n",
    "vix_data['Change'] = vix_data['Close'].pct_change()  # Daily percentage change\n",
    "vix_data['PrevChange'] = vix_data['Change'].shift(1)\n",
    "\n",
    "# Define Reversal Conditions\n",
    "vix_data['Positive_Reversal'] = (vix_data['PrevClose'] > vix_data['PrevClose_2']) & (vix_data['Close'] < vix_data['PrevClose']) & (vix_data['PrevChange'] > 0.05)\n",
    "vix_data['Negative_Reversal'] = (vix_data['PrevClose'] < vix_data['PrevClose_2']) & (vix_data['Close'] > vix_data['PrevClose']) & (vix_data['PrevChange'] < -0.05)\n",
    "\n",
    "# Label days\n",
    "vix_data['Reversal_Label'] = 'No Reversal'  # Default label\n",
    "vix_data.loc[vix_data['Positive_Reversal'], 'Reversal_Label'] = 'Positive'\n",
    "vix_data.loc[vix_data['Negative_Reversal'], 'Reversal_Label'] = 'Negative'\n",
    "\n",
    "# Save results\n",
    "vix_data = vix_data[['Date', 'Close', 'Reversal_Label']]\n",
    "vix_data.to_csv(\"../data/VIX_reversals.csv\", index=False)\n",
    "pd.concat([vix_data.head(), vix_data.tail()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how quickly traders react to news, I need to decide if I should lag the news headlines.\n",
    "\n",
    "The `mutual_info_classif` function from `sklearn.feature_selection` estimates the mutual information (MI) between each feature and the target variable.\n",
    "\n",
    "Mutual information measures the amount of information one variable provides about another. Higher values indicate stronger relationships.\n",
    "\n",
    "MI is non-negative, meaning it always ranges from 0 to positive values.\n",
    "\n",
    "A value of 0 means the feature and target are independent (no predictive power).\n",
    "\n",
    "Higher MI means the feature contains more information about the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Info for no lag: 0.5686\n",
      "Mutual Info for 1-day lag: 0.0303\n",
      "Mutual Info for 2-day lag: 0.0144\n",
      "Mutual Info for 3-day lag: 0.0122\n",
      "Mutual Info for 4-day lag: 0.0108\n",
      "Mutual Info for 5-day lag: 0.0120\n",
      "Mutual Info for 6-day lag: 0.0058\n",
      "Mutual Info for 7-day lag: 0.0021\n"
     ]
    }
   ],
   "source": [
    "# Read in daily combined headlines\n",
    "headlines = pd.DataFrame()\n",
    "for i in range(1, 11):\n",
    "    headlines = pd.concat([headlines, pd.read_csv(f'../data/headlines_{i}.csv')], axis=0).reset_index(drop=True)\n",
    "headlines = headlines.dropna()\n",
    "\n",
    "# Read in VIX reversals\n",
    "vix_reversals = pd.read_csv('../data/VIX_reversals.csv')\n",
    "\n",
    "# Merge headlines with VIX reversals (test with inner join for now, will forward fill non-trading days later)\n",
    "df_lag_test = headlines.merge(vix_reversals, left_on='date', right_on='Date', how='inner')\n",
    "\n",
    "# Convert labels to numeric (for correlation analysis)\n",
    "label_map = {'No Reversal': 0, 'Positive': 1, 'Negative': 2}\n",
    "df_lag_test['Reversal_Label_Num'] = df_lag_test['Reversal_Label'].map(label_map)\n",
    "\n",
    "# Shift labels to simulate different lags\n",
    "df_lag_test['Label_1DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-1)\n",
    "df_lag_test['Label_2DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-2)\n",
    "df_lag_test['Label_3DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-3)\n",
    "df_lag_test['Label_4DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-4)\n",
    "df_lag_test['Label_5DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-5)\n",
    "df_lag_test['Label_6DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-6)\n",
    "df_lag_test['Label_7DayLag'] = df_lag_test['Reversal_Label_Num'].shift(-7)\n",
    "\n",
    "# Compute Mutual Information\n",
    "X = df_lag_test[['Reversal_Label_Num', 'Label_1DayLag', 'Label_2DayLag', 'Label_3DayLag', 'Label_4DayLag', 'Label_5DayLag', 'Label_6DayLag', 'Label_7DayLag']].dropna()\n",
    "y = df_lag_test['Reversal_Label_Num'].iloc[:len(X)]  # Align y with X\n",
    "\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features=True)\n",
    "print(f\"Mutual Info for no lag: {mutual_info[0]:.4f}\")\n",
    "print(f\"Mutual Info for 1-day lag: {mutual_info[1]:.4f}\")\n",
    "print(f\"Mutual Info for 2-day lag: {mutual_info[2]:.4f}\")\n",
    "print(f\"Mutual Info for 3-day lag: {mutual_info[3]:.4f}\")\n",
    "print(f\"Mutual Info for 4-day lag: {mutual_info[4]:.4f}\")\n",
    "print(f\"Mutual Info for 5-day lag: {mutual_info[5]:.4f}\")\n",
    "print(f\"Mutual Info for 6-day lag: {mutual_info[6]:.4f}\")\n",
    "print(f\"Mutual Info for 7-day lag: {mutual_info[7]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lag (0 days) has the highest MI (0.5686), meaning today's news headlines have the strongest relationship with the VIX reversal label.\n",
    "\n",
    "As the lag increases (1 day, 2 days, etc.), the MI drops sharply, meaning older headlines are less useful in predicting reversals.\n",
    "\n",
    "By 7 days, MI is nearly 0.0021, indicating almost no relationship.\n",
    "\n",
    "This aligns well with my chunking approach as traders can test new chunks at a time and identify if they are presently undergoing a volatility reversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIX only has data on training days. compile the news over non trading days to classify for upcoming trading day.\n",
    "\n",
    "As previously mentioned, the chunking of concatenated news headlines is done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Reversal_Label</th>\n",
       "      <th>combined_headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>WIPO Publishes Patent of DIRECTA PLUS S.P.A. w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>Invesco S&amp;P 500 Downside Hedged (PHDG:$32.61) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>Invesco S&amp;P SmallCap Momentum (XSMO:$49.95) fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>Lyxor S&amp;P 500 UCITS - Daily Hedged D-EUR (SP5H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>iShares Core S&amp;P BSE SENSEX India (2836:HKD35....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>Top Performers Past Week: Palantir Technologie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>S&amp;P 500 Consumer Staples (Sector): The Top Fiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>General Mills offers 46th lowest Price Earning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>SPDR S&amp;P MidCap 400 (MDY:$579.11) in 2nd conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>No Reversal</td>\n",
       "      <td>Consolidated Planning Corp Acquires 4,066 Shar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2980 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Reversal_Label  \\\n",
       "0     2023-11-15    No Reversal   \n",
       "1     2023-11-15    No Reversal   \n",
       "2     2023-11-15    No Reversal   \n",
       "3     2023-11-15    No Reversal   \n",
       "4     2023-11-15    No Reversal   \n",
       "...          ...            ...   \n",
       "2975  2025-02-13    No Reversal   \n",
       "2976  2025-02-13    No Reversal   \n",
       "2977  2025-02-13    No Reversal   \n",
       "2978  2025-02-13    No Reversal   \n",
       "2979  2025-02-13    No Reversal   \n",
       "\n",
       "                                     combined_headlines  \n",
       "0     WIPO Publishes Patent of DIRECTA PLUS S.P.A. w...  \n",
       "1     Invesco S&P 500 Downside Hedged (PHDG:$32.61) ...  \n",
       "2     Invesco S&P SmallCap Momentum (XSMO:$49.95) fa...  \n",
       "3     Lyxor S&P 500 UCITS - Daily Hedged D-EUR (SP5H...  \n",
       "4     iShares Core S&P BSE SENSEX India (2836:HKD35....  \n",
       "...                                                 ...  \n",
       "2975  Top Performers Past Week: Palantir Technologie...  \n",
       "2976  S&P 500 Consumer Staples (Sector): The Top Fiv...  \n",
       "2977  General Mills offers 46th lowest Price Earning...  \n",
       "2978  SPDR S&P MidCap 400 (MDY:$579.11) in 2nd conse...  \n",
       "2979  Consolidated Planning Corp Acquires 4,066 Shar...  \n",
       "\n",
       "[2980 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in daily combined headlines\n",
    "headlines = pd.DataFrame()\n",
    "for i in range(1, 11):\n",
    "    headlines = pd.concat([headlines, pd.read_csv(f'../data/headlines_{i}.csv')], axis=0).reset_index(drop=True)\n",
    "headlines = headlines.dropna()\n",
    "\n",
    "# Read in VIX reversals\n",
    "vix_reversals = pd.read_csv('../data/VIX_reversals.csv')\n",
    "\n",
    "# Merge daily headlines with VIX reversals\n",
    "labelled_news = pd.merge(headlines, vix_reversals, left_on='date', right_on='Date', how='left')\n",
    "# VIX reversals are only available for trading days, so we back fill the NaN Date, then group by back-filled Date and concatenate the headlines\n",
    "labelled_news['Date'] = labelled_news['Date'].bfill()\n",
    "labelled_news = labelled_news.groupby('Date').agg({'combined_headlines': '[SEP]'.join, 'Reversal_Label': 'last'}).reset_index()\n",
    "\n",
    "labelled_news.to_csv('../data/labelled_news.csv', index=False)\n",
    "\n",
    "# I realised that the combined_headlines column likely exceeds FinBERT's token limit, so I will split it into chunks of 100 headlines each which should be within the token limit\n",
    "def split_by_sep(row):\n",
    "    # Split combined_headlines by '[SEP]'\n",
    "    segments = row['combined_headlines'].split('[SEP]')\n",
    "    # Group into chunks of 100\n",
    "    chunks = [segments[i:i + 100] for i in range(0, len(segments), 100)]\n",
    "    return [(row['Date'], row['Reversal_Label'], ' [SEP] '.join(chunk)) for chunk in chunks]\n",
    "expanded_rows = [item for sublist in labelled_news.apply(split_by_sep, axis=1) for item in sublist]\n",
    "chunked_news = pd.DataFrame(expanded_rows, columns=['Date', 'Reversal_Label', 'combined_headlines'])\n",
    "chunked_news.to_csv('../data/chunked_news.csv', index=False)\n",
    "chunked_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reversal_Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Reversal</th>\n",
       "      <td>2522</td>\n",
       "      <td>84.630872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>298</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>160</td>\n",
       "      <td>5.369128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Count  Percentage\n",
       "Reversal_Label                   \n",
       "No Reversal      2522   84.630872\n",
       "Positive          298   10.000000\n",
       "Negative          160    5.369128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "chunked_news = pd.read_csv('../data/chunked_news.csv')\n",
    "\n",
    "# Display the counts and percentages\n",
    "pd.DataFrame({\n",
    "    'Count': chunked_news['Reversal_Label'].value_counts(),\n",
    "    'Percentage': chunked_news['Reversal_Label'].value_counts(normalize=True) * 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am using textual data, instead of oversampling or undersampling, I will adjust the loss function of the model below to give higher weights to the minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed data\n",
    "df = pd.read_csv('../data/chunked_news.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "label_map = {'No Reversal': 0, 'Positive': 1, 'Negative': 2}\n",
    "df['label'] = df['Reversal_Label'].map(label_map)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['combined_headlines'], df['label'], test_size=0.3)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convert the data into torch tensors\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label_ids'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = NewsDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "\n",
    "1. Label Encoding\n",
    "To facilitate supervised learning, the categorical labels (\"No Reversal,\" \"Positive,\" and \"Negative\") are mapped to numerical values: 0: No Reversal, 1: Positive Reversal, 2: Negative Reversal\n",
    "\n",
    "2. Dataset Splitting\n",
    "The dataset is divided into training (70%), validation (15%), and test (15%) subsets using train_test_split. This ensures that the model is trained on one portion of the data while another portion is held out for validation and testing.\n",
    "\n",
    "3. Tokenization Using FinBERT\n",
    "The FinBERT tokenizer (yiyanghkust/finbert-tone) is used to preprocess text data. The tokenizer converts input text into tokenized sequences with attention masks, ensuring a maximum length of 512 tokens to fit BERT's input requirements.\n",
    "\n",
    "Preprocessing into Dataset class:\n",
    "\n",
    "1. Custom PyTorch Dataset\n",
    "A PyTorch Dataset class is defined to handle tokenized data and corresponding labels efficiently. This class:\n",
    "\n",
    "- Stores the tokenized encodings (input_ids, attention_mask).\n",
    "- Converts these encodings into PyTorch tensors.\n",
    "- Provides a structured way to access samples and labels for training.\n",
    "\n",
    "2. Conversion to PyTorch Dataset Format\n",
    "The processed data is encapsulated within the NewsDataset class, which allows efficient batching and loading using PyTorch’s DataLoader for model training and evaluation.\n",
    "\n",
    "This preprocessing ensures that the input data is structured, tokenized, and formatted correctly for training a FinBERT model on volatility reversal classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label to numbers\n",
    "label_map = {'No Reversal': 0, 'Positive': 1, 'Negative': 2} # Use 0, 1, 2 as required by the model. -1, 0, 1 does not work.\n",
    "df['label'] = df['Reversal_Label'].map(label_map)\n",
    "\n",
    "# Splitting the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['combined_headlines'], df['label'], test_size=0.3)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5)\n",
    "\n",
    "# Loading the FinBERT tokenizer for text preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Tokenizing the text data to convert raw text into token IDs and attention masks\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Creating a PyTorch dataset class for handling tokenized data\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Dictionary containing input_ids and attention_mask\n",
    "        self.encodings = encodings\n",
    "        # Corresponding labels for each input\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the encoded values and convert them into PyTorch tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Label tensor\n",
    "        item['label_ids'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return dataset size\n",
    "        return len(self.labels)\n",
    "\n",
    "# Converting tokenized data and labels into dataset objects\n",
    "train_dataset = NewsDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = NewsDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection:\n",
    "\n",
    "A pre-trained FinBERT model (yiyanghkust/finbert-tone) is loaded for sequence classification with three output labels. This model is optimized for financial sentiment analysis, making it well-suited for volatility reversal classification.\n",
    "\n",
    "Training Configuration:\n",
    "\n",
    "- 10 epochs of training\n",
    "- Batch size of 32 for training and evaluation\n",
    "- Cosine learning rate scheduler for gradual decay\n",
    "- Mixed precision (FP16) to speed up training\n",
    "- Early stopping after 3 epochs with no improvement\n",
    "- Model checkpointing at each epoch for best model retrieval\n",
    "\n",
    "Handling Imbalanced Data:\n",
    "\n",
    "Since most data points belong to the No Reversal class, class weights are computed using `compute_class_weight` to assign higher weights to minority classes (Positive and Negative Reversals). These weights are passed to `CrossEntropyLoss` during training. A custom `Trainer` class is implemented to modify the loss function by incorporating class weights. This prevents the model from being biased toward the dominant class.\n",
    "\n",
    "Model Training and Evaluation:\n",
    "- The model is trained using the weighted loss function.\n",
    "- After each epoch, it is evaluated on the validation set.\n",
    "- The best-performing model is saved and tested on the test dataset to assess final performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained FinBERT model for sequence classification with 3 labels (No Reversal, Positive, Negative)\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "\n",
    "# Enable CuDNN optimization for faster training on compatible hardware\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# I ran this on Colab, so I mounted my Google Drive to save the model checkpoints\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# Define the training arguments for model fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/finbert_checkpoints', # Directory to save the model checkpoints\n",
    "    num_train_epochs=10, # Train the model for 10 epochs\n",
    "    per_device_train_batch_size=32, # Batch size for training\n",
    "    per_device_eval_batch_size=32, # Batch size for evaluation\n",
    "    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
    "    max_steps=1000, # Maximum number of training steps (early stopping may halt it sooner)\n",
    "    weight_decay=0.01, # L2 regularization for better generalization\n",
    "    logging_dir='./logs', # Directory for logging training metrics\n",
    "    logging_steps=10, # Log training metrics every 10 steps\n",
    "    eval_strategy=\"epoch\", # Evaluate the model at the end of every epoch\n",
    "    save_strategy=\"epoch\", # Save the model at the end of every epoch\n",
    "    report_to=\"none\", # Disable automatic reporting (e.g., to Weights & Biases)\n",
    "    lr_scheduler_type=\"cosine\", # Use a cosine learning rate scheduler\n",
    "    learning_rate=1e-6, # Initial learning rate\n",
    "    fp16 = True, # Enable mixed-precision training for better performance\n",
    "    load_best_model_at_end=True # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# Compute class weights to handle imbalanced data\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", # Compute inverse class frequencies to balance the dataset\n",
    "    classes=np.array([0,1,2]), # Labels: No Reversal (0), Positive (1), Negative (2)\n",
    "    y=train_labels.to_numpy() # Extract labels from training data\n",
    ")\n",
    "\n",
    "# Convert class weights to a tensor and move to GPU for training\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "# Define a custom Trainer class with weighted loss computation\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")  # Extract labels from inputs\n",
    "        outputs = model(**inputs)  # Forward pass through the model\n",
    "        logits = outputs.get(\"logits\")  # Extract logits (raw model predictions)\n",
    "\n",
    "        # Apply CrossEntropyLoss with computed class weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss  # Return loss (and outputs if needed)\n",
    "\n",
    "\n",
    "# Create the Trainer with the customized loss function\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,  # Use the pre-trained FinBERT model\n",
    "    args=training_args,  # Training configurations\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=val_dataset,  # Validation dataset\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop training if no improvement after 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model using the training dataset\n",
    "trainer.train()\n",
    "# Evaluate the model using the validation dataset\n",
    "trainer.evaluate()\n",
    "\n",
    "# Evaluate the model using the test dataset and print results\n",
    "print(\"Evaluating on the test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/training process.png\">\n",
    "\n",
    "Training was ran on Colab, this is a screenshot of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics & Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: The percentage of correctly classified instances.\n",
    "\n",
    "\n",
    "Precision: Measures how many of the predicted positive cases were actually correct.\n",
    "\n",
    "\n",
    "Recall: Measures how many of the actual positive cases were correctly identified.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, balancing both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Get predicted class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Evaluating on the test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/evaluation results.png\">\n",
    "\n",
    "Metrics was calculated on Colab. This is a screenshot of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned FinBERT model demonstrates a moderate performance in classifying volatility reversals based on daily news headlines. \n",
    "\n",
    "Accuracy (34.68%): This metric indicates that the model correctly predicts the reversal labels for about one-third of the test set. While this is a modest result, it provides a baseline for further improvements.\n",
    "\n",
    "Precision (72.44%): Precision measures the proportion of true positive predictions among all positive predictions. A precision score of 72.44% suggests that when the model predicts a reversal, it is correct approximately 72% of the time. This is important for minimizing false positives, which can lead to unnecessary trading actions.\n",
    "\n",
    "Recall (34.68%): Recall measures the proportion of actual reversals that the model correctly identifies. A recall score of 34.68% indicates that the model successfully identifies about 35% of all actual reversals, highlighting the need for better recall to capture more significant market shifts.\n",
    "\n",
    "F1 Score (42.59%): The F1 score balances precision and recall, providing a single metric that reflects the model's overall performance. A score of 42.59% indicates a moderate performance, balancing the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging FinBERT and fine-tuning it on this specific task, we have created a model that can assist traders and investors in making more informed decisions by anticipating significant shifts in market volatility.\n",
    "\n",
    "The moderate accuracy of the FinBERT model in predicting volatility reversals suggests that while the model has some predictive power, there is room for improvement. Leveraging natural language processing (NLP) techniques in financial analysis remains valuable, as the model's high precision indicates that it can reliably identify true reversals when it makes a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work could explore further improvements, such as\n",
    "1. Experimenting with different model architectures such as LSTM to capture temporal dependencies as previous days' volatility and news may affect future volatility.\n",
    "2. Incorporating additional features such as other financial metrics like trading volume, interest rates, forex. This will likely further improve the model's predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being proactive to seek out the Refinitiv Data Platform gave me access to trustable and relatively clean sources of news.\n",
    "\n",
    "I also have previously read up on financial concepts and I believe it has helped me contextualise the project well, especially helping me to carry out exploratory data analysis. It was really interesting to finally marry finance concepts with actual working data science practices.\n",
    "\n",
    "Using FinBERT probably helped the model perform better as it is specialised to financial knowledge. I considered using Longformers allowing me to pass in their entire concatenated daily headlines. However, I prioritised FinBERT for it's specialised knowledge. Perhaps more testing on this could be done.\n",
    "\n",
    "I used the chunking approach as I believe aggregating the news is more useful than having the model learn from individual headlines. From personal experience, headlines confuse me more than it helps me because I'm stuck thinking if that piece is a contrarian view, or if the rest of the market will react in the same way. By aggregating the headlines, the model can learn the other market sentiment, factoring in the presence of contrarian views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing power, especially the lack of access to a local GPU, handicapped the training process. I was constrained by Colab's GPU maximum run times, which after tweaking and re-training the model, quickly reached the limit.\n",
    "\n",
    "Refinitiv Data Platform also limited the amount of news I could extract. Additionally, their dataset only went back to 2023-11-15. Perhaps with more historical data to train on, the model may yield better results.\n",
    "\n",
    "Using the Refinitiv Data Platform was rather challenging as information and instructions are harder to come by given it's a paywalled and typically B2B service. It was difficult to find answers for questions I had on navigating and using the platform to obtain the necessary and relevant data. One challenge was figuring out that the API through VSCode does not work, and that I have to access the API via the Refinitiv Workspace on their Codeblocks notebook app. Another challenge was querying the data as the API had documentation, however the parameter was just \"query\" but no instructions on how to actually write the query. I believe with better querying to obtain more relevant news might yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
